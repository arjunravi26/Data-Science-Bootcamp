{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Naive Bayes is a probabilistic classifier based on Bayes' Theorem, assuming that features are conditionally independent of each other given the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "* Based on a straightforward application of probabilities (Bayes’ Theorem) and basic counting.\n",
    "* The probabilistic framework allows you to explain predictions using prior and likelihood probabilities.\n",
    "* Naive Bayes is computationally efficient as it reduces the complexity by assuming feature independence.\n",
    "* Scales well with large datasets due to its simplicity.\n",
    "* It requires less data to estimate the probabilities\n",
    "* Due to its simplicity, it generalizes well and is less prone to overfitting.\n",
    "* Particularly useful for discrete data such as text or categorical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "* Unrealistic Assumptions: Features are rarely independent in real-world data.\n",
    "* If a feature value does not appear in the training data, it leads to zero probability during predictions(Use Laplace Smoothing to handle this.)\n",
    "* For continuous features, it assumes a Gaussian distribution, which may not hold in many datasets.\n",
    "* Naive Bayes assumes equal weightage for all classes unless explicitly adjusted. Imbalanced datasets can cause biased predictions.(Use proir Probability adjustment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asmuptions**\n",
    "* Feature Independence\n",
    "* The training data should be representative of the real-world data distribution. If the dataset is biased or limited, the model may perform poorly.\n",
    "* All features used for classification must be present during both training and prediction\n",
    "* Each feature contributes independently to the probability of a class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use?**\n",
    "* Text Classification, Sentimental analysis, Spam Detection.\n",
    "* High dimensional data: It preform well with high dimensional data.\n",
    "* Real-Time Predictions: Due to its speed, it’s great for applications requiring real-time results (e.g., fraud detection).\n",
    "* Works well when data is sparse or when there are fewer samples available\n",
    "* Can handle multi-class problems effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When not to Use?**\n",
    "* When features are highly correlated (e.g., pixels in images), the independence assumption breaks down.\n",
    "* It performs poorly when decision boundaries are nonlinear and complex.\n",
    "* Naive Bayes assumes equal class probabilities unless adjusted, which can lead to biased predictions for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Types of Naive Bayes.**\n",
    "* Multinomial Naive Bayes: Best for discrete features like word counts (Bag-of-Words).\n",
    "* Gaussian Naive Bayes: Best for continuous features, assuming a Gaussian distribution.\n",
    "* Bernoulli Naive Bayes: Best for binary features (e.g., presence or absence of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula : \n",
    "\n",
    "C* =\n",
    "argmax\n",
    "​\n",
    " P(C) \n",
    "i=1\n",
    "∏\n",
    "n\n",
    "​\n",
    " P(x \n",
    "i\n",
    "​\n",
    " ∣C) <br>\n",
    " In real world we use <br>\n",
    " logP(C∣X)∝logP(C)+ \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " logP(x \n",
    "i\n",
    "​\n",
    " ∣C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initalize features and target\n",
    "* Calculate joint probability of each feature with target(p(x/y)): Probability of x happen when y already happened.\n",
    "* Use bayesian equation calculate the probability for each class with respect to the feature given\n",
    "* Find the high probability and assign the data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    '''Implement naive bayes from scratch'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.probabilites = {}  # store the join occurance of each feature value and target\n",
    "        self.feature = None  # to store features while fitting\n",
    "        self.target = None  # to store classes while fitting\n",
    "        self.cls_count = {}  # to store count for each classes in the target\n",
    "        self.size = 0  # size of the data\n",
    "        self.classes = 0\n",
    "        self.alpha = 1\n",
    "\n",
    "    def __create_counter(self):\n",
    "        '''Function for counting occurance of feature value with each target'''\n",
    "        cols = self.feature.shape[1]\n",
    "        rows = self.feature.shape[0]\n",
    "        # Iterate through each features in the training data\n",
    "        for feature in range(cols):\n",
    "            # Iterate through each values in that feature\n",
    "            for row_idx in range(rows):\n",
    "                feature_value = str(self.feature[row_idx, feature])\n",
    "                cls_value = self.target[row_idx]\n",
    "                if (feature, feature_value, cls_value) not in self.probabilites:\n",
    "                    self.probabilites[(feature, feature_value, cls_value)] = 0\n",
    "                self.probabilites[(feature, feature_value, cls_value)] += 1\n",
    "\n",
    "    def _cal_prob(self, datas):\n",
    "        if datas.ndim == 1:\n",
    "            datas = datas.reshape(1, -1)\n",
    "        cols = datas.shape[1]\n",
    "        classes = []\n",
    "        # Iterate through each data in the datas\n",
    "        for data in datas:\n",
    "            p_cls = [] # to store the log probability for each classes\n",
    "            # iterate through each classes\n",
    "            for cls in self.cls_count.keys():\n",
    "                likelihood = 0 # set likelihood to 0\n",
    "                # iterate through each features\n",
    "                for feature in range(cols):\n",
    "                    feature_value = str(data[feature]) # feature\n",
    "                    # find the intersection of feature value and class\n",
    "                    count_feature_target = self.probabilites[(feature, feature_value, int(cls))]\n",
    "                    # count the occurance of that class\n",
    "                    count_cls = self.cls_count[cls]\n",
    "                    # cal log likelihood\n",
    "                    likelihood += np.log((count_feature_target + self.alpha) / count_cls + (self.alpha * len(self.classes)))\n",
    "                likelihood += np.log(count_cls/self.size)\n",
    "                # append likelihood of each cls\n",
    "                p_cls.append(likelihood)\n",
    "            # find the max probability\n",
    "            classes.append(self.classes[np.argmax(p_cls)])\n",
    "        return np.array(classes)\n",
    "\n",
    "\n",
    "    def __count_class(self):\n",
    "        for cls in self.target:\n",
    "            cls = str(cls)\n",
    "            if cls not in self.cls_count:\n",
    "                self.cls_count[cls] = 1\n",
    "            else:\n",
    "                self.cls_count[cls] += 1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.feature = X\n",
    "        self.target = y\n",
    "        self.size = X.shape[0]\n",
    "        self.classes = np.unique(self.target)\n",
    "        self.__create_counter()\n",
    "        self.__count_class()\n",
    "\n",
    "    def predict(self, data):\n",
    "        data = np.array(data)\n",
    "        idx = self._cal_prob(data)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature1 = np.random.choice([0, 1, 2], size=100)\n",
    "feature2 = np.random.choice([2, 3, 4], size=100)\n",
    "feature3 = np.random.choice([5, 1, 0], size=100)\n",
    "feature4 = np.random.choice([0, 5, 2], size=100)\n",
    "X = np.column_stack([feature1, feature2, feature3, feature4])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.choice([0, 1], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 0])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(a,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GaussianNB()\n",
    "gb.fit(X,y)\n",
    "p = gb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(p,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
